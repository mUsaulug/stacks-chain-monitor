# Spring Boot 3.2.5 application architecture needs five critical fixes before production

Your Spring Boot application faces **three production-blocking security vulnerabilities**, **two performance bottlenecks** that will cause race conditions at scale, and **one confirmed data integrity issue** with missing Chainhook fields. The most urgent fix: your JWT implementation using HS256 with symmetric keys creates a single point of failure where any key compromise allows attackers to forge authentication tokens. Combined with in-memory rate limiting that fails across multiple instances and race conditions in notification cooldown logic, your system will experience duplicate notifications and bypassed security controls under load. However, your Spring Security configuration disabling CSRF for stateless JWT APIs is architecturally sound—this is the correct pattern per official Spring Security 6.x documentation.

The performance issues stem from caching mutable JPA entities directly with Spring @Cacheable (causing thread safety problems with distributed caches like Redis) and using IDENTITY generation strategy that completely disables Hibernate batch operations. These patterns, while functional in development, will degrade severely under production workloads. The good news: all issues have well-documented solutions with concrete implementation patterns available in official Spring documentation and proven at scale by major applications.

## Critical security vulnerabilities require immediate remediation

**Your JWT implementation must migrate from HS256 to RS256** before any production deployment. The current symmetric key approach means the same secret both signs and verifies tokens—if this secret leaks through configuration files, logs, or compromised instances, attackers can forge arbitrary authentication tokens with any user identity or privilege level. According to OWASP's JWT Cheat Sheet for Java, HS256 is vulnerable to offline brute-force attacks and creates key distribution nightmares in microservices architectures. The recommended RS256 asymmetric approach uses a private key for signing (kept secure on auth server) and public key for verification (safely distributed to all services). Even if the public key is compromised, attackers cannot forge new tokens.

Implementation requires generating **4096-bit RSA key pairs** and updating JJWT 0.12.5 usage to sign with the private key while services verify with the public key. But don't stop there—production JWT security demands three additional layers. First, implement token fingerprinting to prevent token sidejacking: generate a random value, store it in an HttpOnly/Secure cookie, and include its SHA-256 hash in the JWT payload. On each request, verify the cookie value matches the JWT hash. Second, use aggressive 15-minute expiration times for access tokens paired with longer-lived refresh tokens. Third, implement a revocation denylist by storing SHA-256 digests of revoked JWTs in your database with cleanup jobs removing expired entries every 6 hours.

The webhook HMAC validation contains a **timing attack vulnerability** that undermines your signature verification. Standard string comparison with `.equals()` returns immediately on the first mismatched character, leaking timing information that allows attackers to brute-force signatures byte-by-byte. This reduces attack complexity from 2^256 attempts to approximately 65,536 attempts—a massive security degradation. Java provides `MessageDigest.isEqual()` specifically for constant-time cryptographic comparisons. Always compare raw byte arrays, never hex strings. Enhancement: add timestamp validation (reject requests older than 5 minutes) and nonce-based replay protection using Redis to track used nonces with 10-minute TTL.

Your **in-memory rate limiting completely fails in multi-instance deployments**. Each application instance maintains separate token buckets, so three instances with 100 requests/minute limits effectively allow 300 requests/minute. Users can bypass limits by hitting different load balancer backends. The solution requires Redis-backed distributed state using Bucket4j's `LettuceBasedProxyManager`. Configure multi-tier rate limiting: 100 requests per minute for API calls, 20 per second for burst protection, and strict 5 attempts per 15 minutes for login endpoints. The distributed approach ensures all instances share the same rate limit state, providing true protection regardless of deployment topology.

## Performance optimization requires architectural changes to entity caching and batch operations

**Spring @Cacheable with JPA entities creates three critical problems** when using distributed caches like Redis. First, cached entities are mutable objects—one thread modifying a cached entity affects all threads receiving that cached reference, violating thread safety. Second, serializing JPA entities for Redis trips over lazy-loaded associations and Hibernate proxies, causing serialization exceptions. Third, entities retrieved from cache are detached from the persistence context, leading to confusing LazyInitializationExceptions and lost updates. According to Vlad Mihalcea's definitive analysis, the claim "O(1) lookup is not real" has merit: while cache hits achieve O(1), the multi-tier checking (first-level cache → second-level cache → query cache → database) adds overhead, and distributed caches introduce network latency.

The correct pattern depends on your caching goals. For entity caching, **use Hibernate second-level cache instead of Spring @Cacheable**. The second-level cache stores dehydrated entity state (Object arrays), not entity references, reconstructing fresh instances for each request—this provides inherent thread safety. Enable it by setting `hibernate.cache.use_second_level_cache=true`, configuring a JCache implementation, and annotating entities with `@Cacheable` and `@Cache(usage = CacheConcurrencyStrategy.READ_WRITE)`. For query result caching, use Spring @Cacheable with immutable DTOs. Thorben Janssen's benchmarks show DTO projections are 40% faster than full entity queries while avoiding persistence context overhead. Project directly to DTOs in JPQL using constructor expressions: `SELECT new com.example.UserDTO(u.id, u.name) FROM User u`.

**Database query optimization for rule matching should combine database indexes with application-level caching**. Your current O(k) loop through all rules of a type becomes problematic as rule counts grow. Create composite database indexes starting with high-cardinality columns: `CREATE INDEX idx_rule_matching ON rules(contract_address, type, asset_id)`. Database indexes provide O(log n) lookup performance—Acceldata's research shows composite indexes deliver 2-5x speedups for multi-column WHERE clauses, with real-world examples improving from 7000ms to 200ms (35x). Complement database indexing with an in-memory index built at startup: use nested Maps grouping rules by type then contract address, reducing from O(k) all rules to O(m) rules per contract (typically m << k). Cache the filtered results using Spring @Cacheable with keys incorporating all match parameters.

**Concurrent notification creation exhibits a classic race condition** in the isInCooldown check followed by markAsTriggered update. Two threads can both check cooldown (both see "not in cooldown"), then both mark as triggered, resulting in duplicate notifications. The production-ready solution requires JPA optimistic locking using the `@Version` annotation. Add a version field to your notification state entity—Hibernate automatically increments this on each UPDATE and includes it in the WHERE clause. If two transactions try to update simultaneously, the second encounters an `OptimisticLockException` when its version no longer matches. Wrap your notification logic in `@Transactional` and add retry logic with `@Retryable` for optimistic lock failures. Alternative: use a conditional UPDATE query that atomically checks cooldown and updates the timestamp only if not in cooldown, returning the row count to indicate success.

**Your batch operations aren't actually batching** due to IDENTITY ID generation strategy. Hibernate needs the generated ID immediately after each INSERT (returned by the database), forcing sequential execution that prevents batching even when `hibernate.jdbc.batch_size` is configured. Switching to SEQUENCE generation with `allocationSize=50` allows Hibernate to pre-allocate IDs, enabling true batching. Real-world benchmarks from DEV Community show dramatic improvements: 10,000 inserts take 185 seconds with IDENTITY but only 9 seconds with SEQUENCE batching (95% improvement), and 4.3 seconds with batch size 1000 (98% improvement). Configure `hibernate.jdbc.batch_size=30`, `hibernate.order_inserts=true`, and for large datasets manually flush and clear the EntityManager every 30 entities to prevent first-level cache memory bloat. For MySQL specifically, add `rewriteBatchedStatements=true` to your JDBC URL.

## Domain model design patterns show trade-offs between performance and database integrity

**Single Table Inheritance for AlertRule provides optimal query performance** at the cost of database-level integrity constraints. All rule subclasses store in one table with a discriminator column, enabling single SELECT queries without JOINs—this is JPA's default strategy for good reason. The downside: subclass-specific columns cannot have NOT NULL constraints since other subclasses leave them null. Mitigate this by using Bean Validation (`@NotNull` on the entity) to enforce constraints at the application layer. This pattern is ideal when subclasses share substantial common structure and query performance is critical.

**Joined Table Inheritance for TransactionEvent offers full normalization** but introduces performance overhead from JOIN operations. Each subclass gets its own table, enabling proper NOT NULL constraints and avoiding sparse tables, but polymorphic queries that fetch all event types require JOINing multiple tables—Baeldung's analysis shows this can be 2-3x slower than Single Table. Optimize by querying specific subclasses directly rather than the parent class, using fetch strategies carefully, and creating appropriate indexes on join columns. Consider projections to specific DTOs to avoid loading unnecessary subclass data.

**Business key equals and hashCode implementation is non-negotiable** for JPA entities used in Sets or compared across sessions. Vlad Mihalcea emphasizes: never use default Object identity, never use database ID alone (it's null for new entities), always use immutable business keys. For AlertRule, use a business key like rule code or unique constraint columns. Implement equals checking class type and business key, and hashCode returning a fixed value like `getClass().hashCode()` (this maintains Set invariants even as business key values change during entity lifecycle). Hibernate's internals depend on consistent hashCode for collection membership.

## DTO and entity separation requires MapStruct with parsers returning DTOs

**Manual DTO-to-entity mapping in controllers creates maintainability debt** that grows linearly with your domain model. Every new field requires updating multiple mapper methods, and maintaining consistency across conversions becomes error-prone. MapStruct eliminates this through compile-time code generation: define a mapper interface with `@Mapper(componentModel = "spring")`, declare methods like `UserDTO toDTO(User entity)`, and MapStruct generates implementation code at compilation. Baeldung's performance benchmarks show MapStruct processes **6.5 million operations per second** versus 500K for reflection-based mappers—over 13x faster while providing compile-time type safety. Configure with version 1.5.5.Final and ensure your build tool runs the annotation processor.

**Parsers should return DTOs, never entities**. Parsers are infrastructure layer components handling external data formats—they should have zero knowledge of JPA or persistence concerns. Returning DTOs maintains clean architectural boundaries: parsers translate external formats to internal DTOs, services map DTOs to entities and execute business logic within transaction boundaries, and repositories handle entity persistence. This separation allows changing persistence technology without touching parsers, enables testing parsers without database infrastructure, and keeps transaction boundaries explicit at the service layer where `@Transactional` annotations belong.

The proper layered architecture flows: **Controllers accept and return DTOs** (defining your API contract), **services map between DTOs and entities** (using MapStruct), **services execute business logic and manage transactions**, and **repositories work exclusively with entities**. Transaction boundaries live at the service layer because that's where business operations complete. Never expose entities to controllers—changes to JPA entities shouldn't ripple to API contracts, and lazy loading fails outside transaction scope anyway.

## Exception handling strategy needs specific exceptions with transaction-aware rollback

**GlobalExceptionHandler with @RestControllerAdvice provides centralized error handling** but only delivers value with specific exception types. Generic RuntimeExceptions obscure intent and prevent granular error responses. Create an exception hierarchy: ResourceNotFoundException for 404s, DuplicateResourceException for 409s, ValidationException for 400s. Each handler method returns consistent ErrorResponse DTOs with appropriate HTTP status codes. Log exceptions with sufficient context for debugging but never expose stack traces or sensitive data in API responses.

**Transaction rollback behavior has critical nuances** documented in official Spring Framework reference. By default, `@Transactional` methods roll back only on `RuntimeException` and `Error`—checked exceptions do NOT trigger rollback. This surprises developers expecting all exceptions to rollback. Use `@Transactional(rollbackFor = Exception.class)` to rollback on any exception, or stick with RuntimeException subclasses for your custom exceptions. Critically: catching exceptions prevents rollback even within a transactional method. If you catch and handle an exception, the transaction commits normally. To trigger rollback after handling, you must re-throw the exception or call `TransactionAspectSupport.currentTransactionStatus().setRollbackOnly()`.

## Testing strategy should emphasize TestContainers with proper test pyramid balance

**TestContainers with PostgreSQL 14 provides true integration testing** against your actual database, catching SQL dialect issues and constraint violations that in-memory H2 databases mask. Use `@Testcontainers` at class level with `@Container` for PostgreSQL, then `@DynamicPropertySource` to inject connection properties into Spring context. Modern TestContainers supports shared containers across test classes (singleton pattern) to amortize startup cost, and `postgres:14-alpine` image provides quick startup times. According to official TestContainers guides, this approach is now standard for Spring Boot REST API testing.

**Test pyramid balance prevents slow test suites** that discourage running tests. Target 70% unit tests for pure business logic using mocks, 20-25% slice tests with `@WebMvcTest` for controllers and `@DataJpaTest` for repositories, and 10-15% full integration tests with `@SpringBootTest`. Resist the temptation to use `@SpringBootTest` for everything—it boots the entire application context, adding 2-5 seconds per test. Slice tests load only relevant infrastructure: `@WebMvcTest` loads Spring MVC but mocks services, `@DataJpaTest` loads JPA but not web layer, both providing fast feedback on focused concerns.

## Chainhook integration has confirmed missing fields and requires idempotency implementation

**BlockMetadataDto is definitively missing critical burn block fields** that Chainhook webhooks provide. Official Hiro documentation confirms the `apply[].metadata` object includes `bitcoin_anchor_block_identifier` with `index` (burn_block_height) and `hash` (burn_block_hash). Additionally, `parent_burn_block_time`, `parent_burn_block_hash`, and `parent_burn_block_height` fields are available per recent stacks-blockchain-api commits. Your DTO should include `burn_block_height`, `burn_block_hash`, `stacks_block_hash` (which differs from index_block_hash shown in explorers), and optionally `burn_block_time` and `consensus_hash` for complete block metadata. Map these from the nested `metadata.bitcoin_anchor_block_identifier` structure in webhook payloads.

**Idempotency implementation is mandatory** because Chainhook (like all webhook providers) operates on "at-least-once" delivery semantics. Network failures, timeouts, and retries mean the same webhook can arrive multiple times, potentially out of order. Without idempotency, you'll create duplicate AlertNotifications, send multiple messages to users, and violate business logic assumptions. The production pattern combines database unique constraints with transactional processing: create a UNIQUE constraint on `(block_height, transaction_hash, event_type)` or a unique `event_id` column, attempt INSERT within a transaction, catch duplicate key violations (PostgreSQL error code 23505), and return 200 OK even for duplicates to prevent infinite retries. This pattern guarantees exactly-once processing regardless of delivery count.

Enhance robustness by storing webhook payloads in a raw events table before processing, enabling replay for debugging. Implement an idempotency key handler that checks Redis for fast duplicate detection (1-2ms lookup) before hitting the database, with 24-hour TTL. Always respond with 200 OK—even for duplicates—because non-2xx responses trigger provider retries. Perform side effects (sending notifications) only after successful database insert within the same transaction. According to Hookdeck's webhook best practices, this pattern is proven at scale by Stripe, Shopify, and other major API providers.

**Null safety concerns for nonce and fee fields have specific resolutions** based on Stacks blockchain documentation. Nonce cannot be null in Stacks transactions—it's mandatory for all transactions, starting at 0 for new accounts and incrementing by 1 for each transaction. If you encounter null nonce values, default to 0 safely as that represents the first transaction for a new account. The fee field in official Stacks transaction format is `fee` (not `fee_units`), measured in microSTX where 1 STX equals 1,000,000 microSTX. If your implementation uses `fee_units`, clarify whether this is a custom field or misnamed standard fee. Fee cannot be null—it's required for all transactions—so validation should reject any transaction missing fee information.

**Event type handling needs type safety through enums or Zod schemas** to prevent "stringly-typed" runtime errors from typos, case sensitivity, or unknown types. Define a TypeScript enum or Zod schema listing all valid event types: `contract_call`, `contract_deployment`, `stx_transfer`, `ft_mint`, `ft_transfer`, `ft_burn`, `nft_mint`, `nft_transfer`, `nft_burn`, `print_event`. Use Zod's discriminated union to validate at parse time, throwing errors for invalid payloads before processing begins. For production systems, implement an event handler registry pattern mapping event types to handler functions, with explicit logic for unknown types: either fail fast with clear errors, route to default handlers, or skip with warning logs. Add circuit breaker logic that stops processing an event type after 10 consecutive failures to prevent cascading issues from malformed payloads.

## Implementation priorities optimize for production readiness impact

Phase 1 critical security fixes must deploy before production: migrate JWT to RS256 with fingerprinting and revocation (preventing authentication bypass), fix HMAC timing attacks with `MessageDigest.isEqual()` (preventing signature brute-force), and deploy Redis-backed distributed rate limiting (preventing DDoS). These vulnerabilities could compromise production immediately.

Phase 2 addresses race conditions and performance: add `@Version` optimistic locking to notification state (preventing duplicates), implement database unique constraints for Chainhook events (ensuring idempotency), switch ID generation from IDENTITY to SEQUENCE (enabling batch operations), and configure Hibernate batching parameters. These fixes prevent data integrity issues and performance degradation under load.

Phase 3 improves maintainability: implement MapStruct for DTO mapping, add missing Chainhook fields to BlockMetadataDto, create specific exception hierarchy with @RestControllerAdvice, set up TestContainers for integration testing, and implement Zod validation for event types. These changes reduce technical debt and improve long-term code quality without blocking immediate deployment.

The highest-impact fixes combine immediate security value with straightforward implementation: RS256 JWT takes 2-3 hours, MessageDigest.isEqual() is a one-line change, database unique constraints are single DDL statements, and @Version annotation is per-entity. These four changes alone eliminate three critical vulnerabilities and one race condition affecting production readiness. Your Spring Security configuration disabling CSRF is already correct—stateless JWT APIs with tokens in Authorization headers don't need CSRF protection per Spring Security 6.x documentation, so no changes required there.

All recommendations draw from authoritative sources: Spring Framework official documentation, Spring Security 6.x reference, OWASP security cheat sheets, Hibernate performance guides by Vlad Mihalcea, JPA best practices from Thorben Janssen, Baeldung performance benchmarks, official Chainhook/Stacks blockchain API documentation, and proven webhook patterns from Hookdeck and major API providers. Every code example follows production patterns used by enterprise Spring Boot applications at scale.